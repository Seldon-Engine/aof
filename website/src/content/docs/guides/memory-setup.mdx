---
title: Memory Module Setup
description: Enable and configure AOF's tiered memory module — embedding providers, search, and memory pools.
sidebar:
  order: 2
---

import { Aside, Steps } from '@astrojs/starlight/components';

AOF includes an optional memory module that provides semantic search across your agent swarm's knowledge base. It's disabled by default to avoid conflicts with OpenClaw's built-in `memory-core` plugin.

<Aside type="note">
If you're using `memory-core` (OpenClaw's built-in memory plugin), you can enable both simultaneously. They use separate storage and separate vector indexes. AOF's memory module is project-scoped; memory-core is system-wide.
</Aside>

## Prerequisites

- An embedding provider: **OpenAI API key** or a **local Ollama instance**
- AOF installed and initialized (`aof init`)
- The memory module is not available in standalone CLI mode without configuration

---

## Step 1: Enable the Module

In `~/.openclaw/openclaw.json`:

```json
{
  "plugins": {
    "aof": {
      "config": {
        "modules": {
          "memory": {
            "enabled": true
          }
        }
      }
    }
  }
}
```

---

## Step 2: Configure the Embedding Provider

<Steps>

1. **OpenAI (recommended for best quality)**

   ```json
   {
     "memory": {
       "embedding": {
         "provider": "openai",
         "model": "text-embedding-3-small",
         "apiKey": "sk-...",
         "dimensions": 1536
       }
     }
   }
   ```

   Models and their dimensions:
   
   | Model | Dimensions | Quality | Cost |
   |-------|-----------|---------|------|
   | `text-embedding-3-small` | 1536 | Good | Low |
   | `text-embedding-3-large` | 3072 | Best | Medium |

2. **Ollama (local, no API costs)**

   First, ensure Ollama is running with an embedding model:
   
   ```bash
   ollama pull nomic-embed-text
   ollama serve    # or: ollama run nomic-embed-text
   ```
   
   Then configure:
   
   ```json
   {
     "memory": {
       "embedding": {
         "provider": "ollama",
         "model": "nomic-embed-text",
         "baseUrl": "http://localhost:11434",
         "dimensions": 768
       }
     }
   }
   ```

</Steps>

---

## Step 3: Configure Memory Pools in Your Org Chart

Add `memoryPools` to your `org/org-chart.yaml`:

```yaml
memoryPools:
  hot:
    path: memory/hot
    description: "Canonical core documentation — always indexed"

  warm:
    - id: engineering
      path: memory/warm/engineering
      description: "Engineering team operational knowledge"
      roles:
        - developer
        - reviewer
        - architect

    - id: per-agent
      path: memory/warm/agents
      description: "Per-agent working memory"
      roles:
        - "*"   # All roles

  cold:
    - memory/cold/archive

  adapter: filesystem
```

---

## Step 4: Create Memory Directory Structure

```bash
mkdir -p memory/{hot,warm/{engineering,agents},cold/archive}
```

---

## Step 5: Generate and Audit Configuration

```bash
# Generate a memory config file from your org chart
aof memory generate

# Audit memory config against org chart (checks for missing pools, mismatched roles)
aof memory audit
```

---

## Step 6: Add Initial Content to Hot Tier

The hot tier is always indexed — put your most important documentation here:

```bash
# Copy core documentation to hot tier
cp docs/architecture.md memory/hot/
cp docs/coding-standards.md memory/hot/
cp docs/api-reference.md memory/hot/
```

AOF will index these on the next scheduler cycle (or restart the gateway).

---

## Step 7: Restart the Gateway

```bash
openclaw gateway restart
```

Verify the memory module initialized:

```bash
openclaw gateway status
# Should show: aof memory module: loaded, index: hot (3 docs, 2048 vectors)
```

---

## Configuring Search

Tune the hybrid search behavior:

```json
{
  "memory": {
    "search": {
      "hybridEnabled": true,
      "vectorWeight": 0.7,
      "bm25Weight": 0.3,
      "maxResults": 10,
      "tierBoost": {
        "hot": 1.0,
        "warm": 0.8,
        "cold": 0.5
      }
    }
  }
}
```

**Tuning guidance:**

- **Higher `vectorWeight`**: Better semantic matching ("how does auth work?")
- **Higher `bm25Weight`**: Better keyword matching ("JWT token rotation")
- **`tierBoost`**: Hot docs are ranked higher — ensures authoritative docs surface first

---

## Adding Extra Index Paths

Index documents outside the managed memory pool directories:

```json
{
  "memory": {
    "indexPaths": [
      "~/Projects/AOF/docs",
      "~/shared-knowledge"
    ]
  }
}
```

---

## Disabling the Memory Module

To disable the memory module without removing configuration:

```json
{
  "modules": {
    "memory": {
      "enabled": false
    }
  }
}
```

Restart the gateway. The memory indexes remain on disk and will be loaded again when re-enabled.

---

## Memory Curation

Promote warm-tier knowledge to hot tier when it becomes canonical:

```bash
# Generate curation tasks (assigns work to the designated curator role)
aof memory curate

# Audit for stale or missing hot-tier content
aof memory audit
```

Define the curator role in your org chart:

```yaml
memoryCuration:
  policyPath: org/curation-policy.yaml
  role: curator
```

See [Tiered Memory Architecture](/concepts/tiered-memory) for details on the tier promotion pipeline.
